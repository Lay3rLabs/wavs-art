###################################
#
# make start-all-local
#
###################################

# docker volume rm $(docker volume ls -q -f "dangling=true")


services:
  warg-server:
    image: "ghcr.io/reecepbcups/warg-registry:v0.9.3"
    container_name: "warg-server"
    platform: linux/amd64
    environment:
      WARG_OPERATOR_KEY: ecdsa-p256:I+UlDo0HxyBBFeelhPPWmD+LnklOpqZDkrFP5VduASk=
      WARG_NAMESPACE: example
      WKG_REGISTRY: http://localhost:5000
      WARG_CONTENT_BASE_URL: http://localhost:8090
      WARG_LISTEN: 0.0.0.0:8090
      WARG_VERBOSE: 1
    ports:
      - 8090:8090
    command: ["--rm"]

  ipfs:
    image: ipfs/kubo:v0.34.1
    container_name: ipfs
    network_mode: host
    ports:
      - "4001:4001"
      - "4001:4001/udp"
      - "8080:8080"
      - "5001:5001"
    stop_signal: SIGKILL
    command: daemon
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    stop_signal: SIGKILL
    environment:
      HSA_OVERRIDE_GFX_VERSION: "11.0.0"
      HIP_VISIBLE_DEVICES: "0"
    # devices:
    #   - "/dev/kfd"
    #   - "/dev/dri"
    security_opt:
      - seccomp:unconfined
    cap_add:
      - SYS_PTRACE
    ipc: host
    group_add:
      - video
    volumes:
      - ./.docker/data_ollama:/root/.ollama
      - ./.docker/ollama.entrypoint.sh:/entrypoint.sh
    networks:
      - llm-network
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    # command: serve
    # Auto-pull models on startup
    entrypoint: ["/bin/bash", "/entrypoint.sh"]

  download-sd1.5:
    build: ./.docker/sd1.5-download
    command: ["/entrypoint.sh", "download-sd1.5"]
    volumes:
      - ./.docker/data_stable-diffusion:/data
    networks:
      - llm-network

  # sd1.5
  sd-api:
    build: ./.docker/AUTOMATIC1111
    tty: true
    stop_signal: SIGKILL
    ports:
      - "7860:7860"
    volumes:
      - ./.docker/data_stable-diffusion:/data
    environment:
      # CPU (slow)
      - CLI_ARGS=--api --no-half --skip-torch-cuda-test --skip-install
      # GPU
      # - CLI_ARGS=--api --skip-install --xformers --no-half-vae --allow-code
      # - NVIDIA_VISIBLE_DEVICES=all # Expose all GPUs to the container [4]
    networks:
      - llm-network
    # deploy:
    #   resources:
    #     # limits:
    #     #   cpus: '4'
    #     #   memory: 16G
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    depends_on:
      - download-sd1.5

networks:
  llm-network:
    driver: bridge
