###################################
#
# make start-all
#
# docker exec -it wavs bash
#
###################################

services:
  wavs:
    image: "ghcr.io/lay3rlabs/wavs:0.4.0-beta.2"
    container_name: "wavs"
    stop_signal: SIGKILL
    network_mode: "host"
    env_file: "./.env"
    ports:
      - "8000:8000"
    environment:
      WAVS_HOME: "/wavs/packages/wavs"
      WAVS_CLI_HOME: "/wavs/packages/cli"
      WAVS_AGGREGATOR_HOME: "/wavs/packages/aggregator"
    command: ["wavs"]
    volumes:
      - "./:/wavs"
      - "./.docker:/root/wavs/cli/"

  aggregator:
    image: "ghcr.io/lay3rlabs/wavs:0.4.0-beta.2"
    depends_on: ["wavs"]
    container_name: "wavs-aggregator"
    stop_signal: SIGKILL
    env_file: "./.env"
    ports:
      - "8001:8001"
    command: ["wavs-aggregator"]
    volumes:
      - ".:/wavs"
    network_mode: "host"

  ipfs:
    image: "ipfs/kubo:v0.34.1"
    container_name: "ipfs"
    stop_signal: SIGKILL
    network_mode: "host"
    ports:
      - "4001:4001"
      - "4001:4001/udp"
      - "8080:8080"
      - "5001:5001"
    command: ["daemon", "--offline"]

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    stop_signal: SIGKILL
    environment:
      HSA_OVERRIDE_GFX_VERSION: "11.0.0"
      HIP_VISIBLE_DEVICES: "0"
    # devices:
    #   - "/dev/kfd"
    #   - "/dev/dri"
    security_opt:
      - seccomp:unconfined
    cap_add:
      - SYS_PTRACE
    ipc: host
    group_add:
      - video
    volumes:
      - ./.docker/data_ollama:/root/.ollama
      - ./.docker/ollama.entrypoint.sh:/entrypoint.sh
    networks:
      - llm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 6G
    # command: serve
    # Auto-pull models on startup
    entrypoint: ["/bin/bash", "/entrypoint.sh"]

  download-sd1.5:
    build: ./.docker/sd1.5-download
    command: ["/entrypoint.sh", "download-sd1.5"]
    volumes:
      - ./.docker/data_stable-diffusion:/data
    networks:
      - llm-network

  # sd1.5
  sd-api:
    build: ./.docker/AUTOMATIC1111
    tty: true
    stop_signal: SIGKILL
    ports:
      - "7860:7860"
    volumes:
      - ./.docker/data_stable-diffusion:/data
    environment:
      # CPU (slow)
      # - CLI_ARGS=--api --skip-install --no-half --precision full --use-cpu all --disable-opt-split-attention --allow-code
      # GPU
      - CLI_ARGS=--api --skip-install --xformers --no-half-vae --allow-code
      - NVIDIA_VISIBLE_DEVICES=all # Expose all GPUs to the container [4]
    networks:
      - llm-network
    deploy:
      resources:
        # limits:
        #   cpus: '4'
        #   memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - download-sd1.5

  # sdxl
  # sd-api:
  #   image: saladtechnologies/sdnext-sdxl10:latest
  #   ports:
  #     - "7860:7860"
  #   environment:
  #     - PORT=7860
  #     - HOST=0.0.0.0
  #   volumes:
  #     - sdxl_data:/root/.cache/sdnext
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   networks:
  #     - llm-network

networks:
  llm-network:
    driver: bridge
